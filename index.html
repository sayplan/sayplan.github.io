<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Visual Language Maps for Robot Navigation</title>
  <meta name="description" content="Visual Language Maps for Robot Navigation">
  <meta name="keywords" content="VLMaps">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:title" content="Visual Language Maps for Robot Navigation">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Visual Language Maps for Robot Navigation">
  <meta property="og:image" content="https://vlmaps.github.io/static/images/cover_lady.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="1082" />
  <meta property="og:image:height" content="639" />
  <meta property="og:url" content="https://vlmaps.github.io" />
  <meta property="og:description" content="Project page for Visual Language Maps for Robot Navigation" />
  <meta name="twitter:title" content="Visual Language Maps for Robot Navigation" />
  <meta name="twitter:description" content="Project page for Visual Language Maps for Robot Navigation" />
  <meta name="twitter:image" content="https://vlmaps.github.io/static/images/cover_lady.png" />

  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
      w[l] = w[l] || []; w[l].push({
        'gtm.start':
          new Date().getTime(), event: 'gtm.js'
      }); var f = d.getElementsByTagName(s)[0],
        j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
          'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
    })(window, document, 'script', 'dataLayer', 'GTM-PFJ2DFW');</script>
  <!-- End Google Tag Manager -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/vlmaps_icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFJ2DFW" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Visual Language Maps for Robot Navigation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.oiermees.com/">Oier Mees</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://andyzeng.github.io/">Andy Zeng</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a><sup>3</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Freiburg University,</span>
              <span class="author-block"><sup>2</sup>Google Research,</span>
              <span class="author-block"><sup>3</sup>University of Technology Nuremberg</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2210.05714.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/vlmaps/vlmaps.git"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- CoLab Link. -->
                <span class="link-block">
                  <a href="https://colab.research.google.com/drive/1xsH9Gr_O36sBZaoPNq1SmqgOOF12spV0?usp=sharing"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <img src="static/images/colab_icon.png" />
                    </span>
                    <span>CoLab</span>
                  </a>
                </span>
                <!-- Google AI Blog Post Link. -->
                <span class="link-block">
                  <a href="https://ai.googleblog.com/2023/03/visual-language-maps-for-robot.html?m=1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <!-- <i class="fab fa-github"></i> -->
                      <img src="static/images/google_blog_post_icon.png" />
                    </span>
                    <span>Google AI Blog</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src=""
                type="video/mp4">
      </video> -->

        <img src="static/images/cover_lady.png" />

        <h2 class="subtitle has-text-centered">
          VLMaps enables spatial goal navigation with language instructions
        </h2>
      </div>
    </div>
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve has-text-centered">
            <video poster="" id="steve video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/back_and_forth_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move back and forth between the box and the keyboard</p>

          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_between_v3_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-chair-tp has-text-centered">
            <video poster="" id="chair-tp video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/right_left_right_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move right 1.5 meters, then move left 3 meters, then move left 1.5 meters</p>
          </div>
          <div class="item item-shiba has-text-centered">
            <video poster="" id="shiba video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_to_plant_x8_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move to the plant</p>
          </div>
          <div class="item item-fullbody has-text-centered">
            <video poster="" id="fullbody video" autoplay controls muted loop playsinline height="100%">
              <source src="static/images/move_in_between_x4_hres_caption.mp4" type="video/mp4">
            </video>
            <p id="overlay">move in between the wooden box and the chair</p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Grounding language to the visual observations of a navigating agent can be
              performed using off-the-shelf visual-language models pretrained on Internet-scale
              data (e.g. image captions). While this is useful for matching images to natural
              language descriptions of object goals, it remains disjoint from the process of
              mapping the environment, so that it lacks the spatial precision of classic geometric maps.
              To address this problem, we propose VLMaps, a spatial map representation that directly
              fuses pretrained visual-language features with a 3D reconstruction of the physical world.
              VLMaps can be autonomously built from video feed on robots using standard exploration
              approaches and enable natural language indexing of the map without additional labeled data.
              Specifically, when combined with large language models (LLMs), VLMaps can be used to
              (i) translate natural language commands into a sequence of open-vocabulary navigation goals
              (which, beyond prior work, can be spatial by construction, e.g. "in between the sofa and TV"
              or "three meters to the right of the chair") directly localized in the map, and (ii) can
              be shared among multiple robots with different embodiments to generate new obstacle maps
              on-the-fly (by using a list of obstacle categories). Extensive experiments carried out
              in simulated and real world environments show that VLMaps enable navigation according
              to more complex language instructions than existing methods.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/images/VLMaps_v24_lt.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <img src="static/images/pipeline.png" />
            <h3 class="title is-4">VLMap Creation</h3>
            <p>
              The key idea behind building a VLMap is to fuse pretrained
              visual-language features into a geometrical reconstruction of the scene. This can be
              done by computing dense pixel-level embeddings from an existing
              visual-language model (over the RGB-D video feed of the robot) and
              back-projecting them onto the 3D surface of the environment (captured
              from depth data used for reconstruction with visual odometry). Finally,
              we generate the top-down scene representation by storing the visual-language
              features of each image pixel in the corresponding grid map pixel location.
            </p>
            <h3 class="title is-4">Open-Vocabulary Landmark Indexing</h3>
            <p>
              We encode the open-vocabulary landmark names ("chair", "green plant", "table" etc.)
              with the text encoder in the Visual Language Model. Then we align the landmark names
              with the pixels in the VLMap by computing the cosine similarity between their embeddings.
              We get the mask of each landmark type with the argmax operation on the similarity score.
            </p>
            <h3 class="title is-4">Navigation Policies Generation</h3>
            <img src="static/images/codegen_notitle.png" />
            <p>
              We generate the navigation policies in the form of executable code with the help of Large Language Models.
              By providing a few examples in the prompt, we exploit GPT-3 to parse language
              instructions into a string of executable code, expressing functions
              or logic structures (if/else statements, for/while loops) and parameterizing API calls
              (e.g., robot.move_to(target_name) or robot.turn(degrees)).
            </p>
            <video autoplay muted loop playsinline width="100%">
              <source src="static/images/vlmaps_blog_post.mp4" type="video/mp4">
            </video>

          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiment</h2>

                <!-- Interpolating. -->
                <h3 class="title is-4">Long-Horizon Spatial Goal Navigation from Language</h3>
                <div class="content has-text-justified">
                  <p>
                    With open-vocabulary landmark indexing, VLMaps enables long-horizon spatial goal navigation with
                    natural language instructions
                  </p>
                </div>
                <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 1</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_1_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_1.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div>


                <div class="columns is-vcentered interpolation-panel">
                  <p>Sequence 2</p>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_2_v1.mp4" type="video/mp4">
                    </video>
                    <p>VLMaps</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/spatial_goal_nav_cow_2.mp4" type="video/mp4">
                    </video>
                    <p>CLIP on Wheels</p>
                  </div>
                </div>


                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Multi-Embodiment Navigation</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>

                <p>Move to the laptop and the box sequentially</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_2_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the window</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_3_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <p>Move to the television</p>
                <br>
                <div class="columns is-vcentered interpolation-panel">
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_drone.mp4" type="video/mp4">
                    </video>
                    <p>Drone</p>
                  </div>
                  <div class="column  has-text-centered">
                    <video autoplay controls muted loop playsinline height="100%">
                      <source src="static/images/multi_4_locobot.mp4" type="video/mp4">
                    </video>
                    <p>LoCoBot</p>
                  </div>
                </div>

                <!-- <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
                <!--/ Re-rendering. -->

              </div>
            </div>
            <!--/ Animation. -->


            <!-- Concurrent Work. -->
            <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
            <!--/ Concurrent Work. -->

          </div>
      </section>


      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code> @inproceedings{huang23vlmaps,
          title={Visual Language Maps for Robot Navigation},
          author={Chenguang Huang and Oier Mees and Andy Zeng and Wolfram Burgard},
          booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
          year={2023},
          address = {London, UK}
          } </code></pre>
        </div>
      </section>


      <section class="section">
        <div class="container is-max-desktop">
          <h2 class="title is-3">People</h2>
          <div class="columns container">
            <div class="column has-text-centered profile">
              <a href="http://www2.informatik.uni-freiburg.de/~huang/"><img src="static/images/huang.jpg"
                  alt="Chenguang Huang" /></a>
              <h3><a href="http://www2.informatik.uni-freiburg.de/~huang/">Chenguang Huang</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="http://www.oiermees.com"><img src="static/images/mees.jpg" alt="Oier Mees" /></a>
              <h3><a href="http://www.oiermees.com">Oier Mees</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="https://andyzeng.github.io/"><img src="static/images/andy.jpg" alt="Andy Zeng" /></a>
              <h3><a href="https://andyzeng.github.io/">Andy Zeng</a></h3>
            </div>

            <div class="column has-text-centered profile">
              <a href="https://www.utn.de/1/wolfram-burgard/"><img src="static/images/burgard.jpg"
                  alt="Wolfram Burgard" /></a>
              <h3><a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a></h3>
            </div>

          </div>

        </div>
      </section>


      <footer class="footer">
        <div class="container">
          <div class="content has-text-centered">
            <a class="icon-link" href="https://arxiv.org/pdf/2210.05714.pdf">
              <i class="fas fa-file-pdf"></i>
            </a>
            <a class="icon-link" href="" class="external-link" disabled>
              <i class="fab fa-github"></i>
            </a>
          </div>
          <div class="columns is-centered">
            <div class="column is-8">
              <div class="content">
                <p> Website borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> under a <a
                    href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                    International</a>
                </p>

              </div>
            </div>
          </div>
        </div>
      </footer>

</body>

</html>